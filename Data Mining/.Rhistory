p.prop <- 2 * pnorm(prop)
#Pepsi Cola example
p.bar <- .464
p <- 0.5
n <- 250
prop <- (p.bar - p)/sqrt((p * (1-p))/250)
p.prop <- 2 * pnorm(prop)
x.bar <- 41100
pop.sd <- 4500
conf.level <- 0.95
z <- 1.96
n <- 36
#We use a little trick here, c() allows us to combine values
CI <- x.bar + c(-z, z) * (pop.sd/sqrt(n))
round(CI, 2)
x.bar <- 5000
pop.sd <- 1500
conf.level <- 0.95
z <- 1.959
n <- 179
#We use a little trick here, c() allows us to combine values
CI <- x.bar + c(-z, z) * (pop.sd/sqrt(n))
round(CI, 2)
x.bar <- 5000.0
pop.sd <- 1500.0
conf.level <- 0.95
z <- 1.959
n <- 179.0
#We use a little trick here, c() allows us to combine values
CI <- x.bar + c(-z, z) * (pop.sd/sqrt(n))
round(CI, 2)
x.bar <- 5000.0
pop.sd <- 1500.0
conf.level <- 0.95
z <- 1.959963985
n <- 179.0
#We use a little trick here, c() allows us to combine values
CI <- x.bar + c(-z, z) * (pop.sd/sqrt(n))
round(CI, 2)
x.bar <- 5000.0
pop.sd <- 1500.0
conf.level <- 0.95
z <- 1.959963985
n <- 179.0
#We use a little trick here, c() allows us to combine values
CI <- x.bar + c(-z, z) * (pop.sd/sqrt(n))
round(CI, 2)
x.bar <- 5000.0
pop.sd <- 1500.0
conf.level <- 0.95
z <- 1.959963985
n <- 179.0
#We use a little trick here, c() allows us to combine values
CI <- x.bar + c(-z, z) * (pop.sd/sqrt(n))
round(CI, 3)
x.bar <- 18.2
sample.sd <- 6.3
conf.level <- 0.90
t <- abs(qt(.05/2, 90))
n <- 91
CI <- x.bar + c(-t, t) * sample.sd/sqrt(n)
round(CI, 2)
x.bar <- 18.2
sample.sd <- 6.3
conf.level <- 0.90
t <- abs(qt(.05/2, 90))
n <- 91
CI <- x.bar + c(-t, t) * sample.sd/sqrt(n)
round(CI, 2)
x.bar <- 18.2
sample.sd <- 6.3
conf.level <- 0.90
t <- abs(qt(.10/2, 90))
n <- 91
CI <- x.bar + c(-t, t) * sample.sd/sqrt(n)
round(CI, 2)
x.bar <- 18.2
sample.sd <- 6.3
conf.level <- 0.90
n <- 91
t <- abs(qt(.10/2, (n-1)))
CI <- x.bar + c(-t, t) * sample.sd/sqrt(n)
round(CI, 2)
x.bar <- 18.2
sample.sd <- 6.3
conf.level <- 0.90
alpha <- 1 - conf.level
n <- 91
t <- abs(qt(.10/2, (n-1)))
CI <- x.bar + c(-t, t) * sample.sd/sqrt(n)
round(CI, 2)
x.bar <- 18.2
sample.sd <- 6.3
conf.level <- 0.90
alpha <- 1 - conf.level
n <- 91
t <- abs(qt(alpha/2, (n-1)))
CI <- x.bar + c(-t, t) * sample.sd/sqrt(n)
round(CI, 2)
x.bar <- 5000.0
pop.sd <- 1500.0
conf.level <- 0.95
z <- 1.959963985
n <- 179.0
e <- z * (pop.sd/sqrt(n))
#We use a little trick here, c() allows us to combine values
CI <- x.bar + c(-z, z) * (pop.sd/sqrt(n))
round(CI, 3)
pop.sd <- .15
conf.level <- 0.95
z <- 1.959963985
n <- 300
e <- z * (pop.sd/sqrt(n))
pop.sd <- .15
conf.level <- 0.95
z <- 1.959963985
n <- 300
e <- z * (pop.sd/sqrt(n))
round(e, 3)
x.bar <- 120
pop.sd <- 20
conf.level <- 0.98
z <- 2.326347874
n <- 50
e <- z * (pop.sd/sqrt(n))
round(e, 3)
#We use a little trick here, c() allows us to combine values
CI <- x.bar + c(-z, z) * (pop.sd/sqrt(n))
round(CI, 3)
p.bar <- 14/150
n <- 150
conf.level <- 0.90
alpha <- 1 - conf.level
e <- t * sqrt((p.bar * (1-p.bar))/n)
round(e, 3)
p.bar <- 14/150
n <- 150
conf.level <- 0.95
z <- 1.644853627
e <- z * sqrt((p.bar * (1-p.bar))/n)
round(e, 3)
p.bar <- 14/150
n <- 150
conf.level <- 0.95
z <- 1.644853627
e <- z * sqrt((p.bar * (1-p.bar))/n)
round(e, 4)
p.bar <- 14/150
n <- 150
conf.level <- 0.95
z <- 1.959963985
e <- z * sqrt((p.bar * (1-p.bar))/n)
round(e, 4)
conf.level <- 0.95
z <- 1.959963985
ul <- .51
ll <- .45
e <- (ul - ll) /2
p.bar <- ul - e
n = sqrt(z) * (p.bar(1-p.bar)) / sqrt(e)
conf.level <- 0.95
z <- 1.959963985
ul <- .51
ll <- .45
e <- (ul - ll) /2
p.bar <- ul - e
n = sqrt(z) * (p.bar * (1-p.bar)) / sqrt(e)
conf.level <- 0.95
z <- 1.959963985
ul <- .51
ll <- .45
e <- (ul - ll) /2
p.bar <- ul - e
n = sqrt(z) * (p.bar * (1-p.bar)) / sqrt(e)
round(n,0)
conf.level <- 0.95
z <- 1.959963985
ul <- .51
ll <- .45
e <- (ul - ll) /2
p.bar <- ul - e
n <- sqrt(z) * (p.bar * (1-p.bar)) / sqrt(e)
round(n,0)
conf.level <- 0.95
z <- 1.959963985
ul <- .51
ll <- .45
e <- (ul - ll) /2
p.bar <- ul - e
n <- (sqrt(z) * (p.bar * (1-p.bar))) / sqrt(e)
round(n,0)
conf.level <- 0.95
z <- 1.959963985
ul <- .51
ll <- .45
e <- (ul - ll) /2
p.bar <- ul - e
n <- (z*z) * (p.bar * (1-p.bar))) / (e*e)
round(n,0)
conf.level <- 0.95
z <- 1.959963985
ul <- .51
ll <- .45
e <- (ul - ll) /2
p.bar <- ul - e
n <- (z*z) * (p.bar * (1-p.bar)) / (e*e)
round(n,0)
conf.level <- 0.95
z <- 1.959963985
ul <- .51
ll <- .45
e <- (ul - ll) /2
p.bar <- ul - e
n <- (z*z) * (p.bar * (1-p.bar)) / (e*e)
ceiling(n)
salaries <- read_xlsx("Starting_Salaries.xlsx", sheet = "Data")
t.test(salaries$`Chemical Engineering`, salaries$`Electrical Engineering`)
t.test
library(readxl); library(tidyverse)
install.packages(readxl)
install.packages(tidyverse)
getwd()
install.packages("tidyverse")
install.packages("readxl")
library(readxl); library(tidyverse)
#We'll do both of these as two-tailed tests
#type ?t.test in the Console to get more details on how to run one-tailed test or test a difference other than 0
mileage <- read_xlsx("Mileage.xlsx", sheet = "Data")
t.test(mileage$`Highway Mileage`, mileage$`City Mileage`)
#Install and load the packages needed for this analysis
#install.packages("readxl"); install.packages("tidyverse")
library(readxl); library(tidyverse)
#We'll do both of these as two-tailed tests
#type ?t.test in the Console to get more details on how to run one-tailed test or test a difference other than 0
mileage <- read_xlsx("Mileage.xlsx", sheet = "Data")
t.test(mileage$`Highway Mileage`, mileage$`City Mileage`)
install.packages("readxl"); install.packages("tidyverse")
mileage <- read_xlsx("Mileage.xlsx", sheet = "Data")
install.packages("MASS"); library(MASS)
# 1. Import groceries.csv file
library(arules)
groceries <- read.transactions("groceries.csv", sep = ",")
#In this lab, we will perform a market basket analysis of transactional data
#from a grocery store.Our market basket analysis will utilize the purchase data
#collected from one month of operation at a real-world grocery store. The data contains
#9,835 transactions.
### ------------------------------------------------------------------------------
setwd('C:/git/repo/R/Data Mining')
groceries <- read.transactions("groceries.csv", sep = ",")
# 2. Understanding of your data.
# Summary of dataset
summary(groceries)
# Inspect the first 5 transactions
inspect(groceries[1:5])
# How many transactions and items in this data?
nrow(groceries)
# 3. Data exploration
# Examine the relative frequency of items in descending order
sort(itemFrequency(groceries, type="relative"), decreasing = TRUE)
# What are the top 3 most frequent items?
sort(itemFrequency(groceries, type="relative"), decreasing = TRUE)[1:3]
# Examine the absolute frquency of items in descending order
sort(itemFrequency(groceries, type="absolute"), decreasing = FALSE)
itemFrequency(groceries)
items(groceries)
# How many transactions and items in this data?
nrow(groceries)
# How many transactions and items in this data?
items(groceries)
# Plot the most frequent 8 items in the descending order of transaction frequency in percentage
itemFrequencyPlot(groceries, type="relative", topN = 8)
# 4. Use the apriori command to generate rules with minimal support = 0.01 and minimal confidence = 0.3 and max length = 2.
groceries_rules <- apriori(groceries, parameter = list(support = 0.01, confidence = 0.3, maxlen=2))
summary(groceries_rules)
# Display all rules sorted by confidence levels.
inspect(sort(groceries_rules, by = "confidence"))
# Display top 5 rules
inspect(sort(groceries_rules, by = "confidence")[1:5])
# 3. Data exploration
# Examine the relative frequency of items in descending order
sort(itemFrequency(groceries, type="relative"), decreasing = TRUE)
summary(groceries_rules)
# Display top 5 rules
inspect(sort(groceries_rules, by = "confidence")[1:5])
# 5. Use the apriori command to  generate rules with minimal support = 0.02 and minimal confidence = 0.4 and max length = 3.
groceries_rules2 <- apriori(groceries, parameter = list(support = 0.02, confidence = 0.4, maxlen=3))
summary(groceries_rules2)
# Display top 10 rules for Task 2 sorted by lift.
inspect(sort(groceries_rules2, by = "confidence")[1:10])
# Display top 10 rules for Task 2 sorted by lift.
inspect(sort(groceries_rules2, by = "lift")[1:10])
# Find and display rules containing "other vegetables"
vegetable_rules <- subset(groceries_rules2, items %in% "other vegetables")
# Find and display rules containing "other vegetables" on the left-hand side
vegetable_rules_l <- subset(groceries_rules2, lhs %in% "other vegetables")
inspect(vegetable_rules_l)
# Find and display rules containing "other vegetables"
vegetable_rules <- subset(groceries_rules2, items %in% "other vegetables")
inspect(vegetable_rules)
# Find and display rules containing "other vegetables" on the right-hand side
vegetable_rules_r <- subset(groceries_rules2, rhs %in% "other vegetables")
inspect(vegetable_rules_r)
# 6. Use the apriori command to generate about 30 to 50 association rules. Set your own minimum support and confidence threshold levels.
# Remember if the thresholds are too low, you will get too many rules, or if you set them too high, you may not get any or enough rules.
groceries_rules3 <- apriori(groceries, parameter = list(support = 0.02, confidence = 0.4, maxlen=4))
summary(groceries_rules3)
# 6. Use the apriori command to generate about 30 to 50 association rules. Set your own minimum support and confidence threshold levels.
# Remember if the thresholds are too low, you will get too many rules, or if you set them too high, you may not get any or enough rules.
groceries_rules3 <- apriori(groceries, parameter = list(support = 0.02, confidence = 0.2, maxlen=4))
summary(groceries_rules3)
# 6. Use the apriori command to generate about 30 to 50 association rules. Set your own minimum support and confidence threshold levels.
# Remember if the thresholds are too low, you will get too many rules, or if you set them too high, you may not get any or enough rules.
groceries_rules3 <- apriori(groceries, parameter = list(support = 0.02, confidence = 0.3, maxlen=4))
summary(groceries_rules3)
# Inspect all of the rules in the descending lift values of the rules.
inspect(groceries_rules3)
# Inspect all of the rules in the descending lift values of the rules.
inspect(sort(groceries_rules, by = "lift"))
# Display top 10 rules for Task 2 sorted by lift.
inspect(sort(groceries_rules2, by = "lift")[1:10])
# Find and display rules containing "other vegetables" on the left-hand side
vegetable_rules_l <- subset(groceries_rules2, lhs %in% "other vegetables")
inspect(vegetable_rules_l)
### 1. Import and clean data
# A. Import data. Load character variable as character strings first (stringsAsFactors = FALSE).
BartRider <- read.csv(file = "BartRider.csv", stringsAsFactors = FALSE)
# B.	Show the overall structure and summary of the input data.
str(BartRider)
summary(BartRider)
# C.	Transform DualInc, Gender, Language, OwnRent, and Rider to factor variables. Show the overall structure and summary of the input data again.
BartRider$DualInc <- factor(BartRider$DualInc)
BartRider$Gender <- factor(BartRider$Gender)
BartRider$Language <- factor(BartRider$Language)
BartRider$OwnRent <- factor(BartRider$OwnRent)
BartRider$Rider <- factor(BartRider$Rider)
str(BartRider)
summary(BartRider)
### 2. Data partitioning and inspection code
# A.	Partition the data set for simple hold-out evaluation - 70% for training and the other 30% for testing.
library(caret)
set.seed(100)
train_index <- createDataPartition(BartRider$Rider, p=0.7, list=FALSE)
datTrain <- BartRider[train_index,]
install.packages("caret")
### 2. Data partitioning and inspection code
# A.	Partition the data set for simple hold-out evaluation - 70% for training and the other 30% for testing.
library(caret)
set.seed(100)
train_index <- createDataPartition(BartRider$Rider, p=0.7, list=FALSE)
datTrain <- BartRider[train_index,]
datTest <- BartRider[-train_index,]
# B.	Show the overall structure and summary of train and test sets. Show the distributions of Rider in the entire set, the train set and the test set.
str(datTrain)
summary(datTrain)
str(datTest)
summary(datTest)
prop.table(table(BartRider$Rider))
prop.table(table(datTrain$Rider))
prop.table(table(datTest$Rider))
### 3. SVM for classification (5 points)
# A. Build a SVM model with C=5
library(kernlab)
library(rminer)
svm_model <- ksvm(Rider~.,data=datTrain)
svm_model
# B. Generate this model's confusion matrices and classification evaluation metrics in training and testing sets.
prediction_on_trainSVM <- predict(svm_model, datTrain)
prediction_on_testSVM <- predict(svm_model, datTest)
mmetric(datTrain$Rider,prediction_on_trainSVM, metric="CONF")
mmetric(datTest$Rider,prediction_on_testSVM, metric="CONF")
mmetric(datTrain$Rider,prediction_on_trainSVM,metric=c("ACC","PRECISION","TPR","F1"))
mmetric(datTest$Rider,prediction_on_testSVM,metric=c("ACC","PRECISION","TPR","F1"))
### 4. Neural Network for classification (6 points)
# A. Build a MLP model with N=100, H='8, 4'
library(RWeka)
MLP <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
# 1. Import the datadet
insurance <- read.csv(file = "insurance.csv", stringsAsFactors = FALSE)
# 2. str() shows the structure of data
str(insurance)
# 3. summary() shows the mean and the five-number statistics indicating the spread of each column's values
summary(insurance)
# 4. Change all categorical variables to factors
insurance$sex <- factor(insurance$sex)
insurance$smoker <- factor(insurance$smoker)
insurance$region <- factor(insurance$region)
str(insurance)
summary(insurance)
# 1. Import the datadet
carAuction <- read.csv(file = "carAuction.csv", stringsAsFactors = FALSE)
# 2. str() shows the structure of data
str(carAuction)
# 3. summary() shows the mean and the five-number statistics indicating the spread of each column's values
summary(carAuction)
# 4. Change all categorical variables to factors
carAuction$Auction <- factor(carAuction$Auction)
carAuction$Color <- factor(carAuction$Color)
carAuction$IsBadBuy <- factor(carAuction$IsBadBuy)
carAuction$Size <- factor(carAuction$Size)
carAuction$TopThreeAmericanName <- factor(carAuction$TopThreeAmericanName)
carAuction$WheelType <- factor(carAuction$WheelType)
str(carAuction)
summary(carAuction)
# 1. Import the datadet
carAuction <- read.csv(file = "carAuction.csv", stringsAsFactors = FALSE)
# 2. str() shows the structure of data
str(carAuction)
# 3. summary() shows the mean and the five-number statistics indicating the spread of each column's values
summary(carAuction)
# 4. Change all categorical variables to factors
carAuction$Auction <- factor(carAuction$Auction)
carAuction$Color <- factor(carAuction$Color)
carAuction$IsBadBuy <- factor(carAuction$IsBadBuy)
carAuction$Size <- factor(carAuction$Size)
carAuction$TopThreeAmericanName <- factor(carAuction$TopThreeAmericanName)
carAuction$WheelType <- factor(carAuction$WheelType)
str(carAuction)
summary(carAuction)
# 5. Partition the dataset: 70% for training, 30% for testing
library(caret)
set.seed(1)
train_index <- createDataPartition(carAuction$IsBadBuy, p=0.7, list=FALSE)
datTrain <- carAuction[train_index,]
datTest <- carAuction[-train_index,]
# 6. Check the rows and porportion of target variable for both training and testing datasets
nrow(datTrain)
nrow(datTest)
prop.table(table(datTrain$IsBadBuy))
prop.table(table(datTest$IsBadBuy))
# 7. Build MLP model with default setting
# MLP's default parameter values of MLP,L=0.3,M=0.2, N=500,H='a'
# L: learning rate with default=0.3
# M: momemtum with default=0.2
# N: number of epochs with default=500
# H <comma seperated numbers for nodes on each layer>
#The hidden nodes to be created on each layer:
# an integer, or the letters 'a' = (attribs + classes) / 2,
#'i' = attribs, 'o' = classes, 't' = attribs + classes)
#for wildcard values, Default = 'a').
library(RWeka)
library(rminer)
MLP <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
mlp_model <- MLP(IsBadBuy~.,data=datTrain)
summary(mlp_model)
# Make predictions on both training and tessting sets
prediction_on_train <- predict(mlp_model, datTrain)
prediction_on_test <- predict(mlp_model, datTest)
# Generating evaluation metrics on both training and testing data
mmetric(datTrain$IsBadBuy,prediction_on_train, metric="CONF")
mmetric(datTest$IsBadBuy,prediction_on_test, metric="CONF")
mmetric(datTrain$IsBadBuy,prediction_on_train,metric=c("ACC","PRECISION","TPR","F1"))
mmetric(datTest$IsBadBuy,prediction_on_test,metric=c("ACC","PRECISION","TPR","F1"))
# 8. Build MLP model contains two hidden layers: 16 hidden nodes for the first layer, and 8 hidden nodes for the second layer. Set N = 100
mlp_model2 <-  MLP(IsBadBuy~.,data=datTrain, control = Weka_control(N=100, H='16, 8'))
summary(mlp_model2)
### 1. Import and clean data
# A. Import data. Load character variable as character strings first (stringsAsFactors = FALSE).
Sales <- read.csv(file = "sales.csv", stringsAsFactors = FALSE)
# B.	Show the overall structure and summary of the input data.
str(Sales)
summary(Sales)
# C. Remove the Name column
Sales$Name <- NULL
# D. Transform Platform, Genre, and Rating to factor variables. Show the overall structure and summary of the input data again.
Sales$Platform <- factor(Sales$Platform)
Sales$Genre <- factor(Sales$Genre)
Sales$Rating <- factor(Sales$Rating)
str(Sales)
summary(Sales)
### 2. Data partitioning
# A.	Partition the data set for simple hold-out evaluation - 70% for training and the other 30% for testing. (1 points)
library(caret)
set.seed(1)
train_index <- createDataPartition(BartRider$Rider, p=0.7, list=FALSE)
train_index <- createDataPartition(Sales$Global_Sales, p=0.7, list=FALSE)
datTrain <- Sales[train_index,]
datTest <- Sales[-train_index,]
### 3. SVM for predicting Global_Sales value. (2 points)
# A. Build a SVM model with C=5.
library(kernlab)
library(rminer)
svm_model <- ksvm(Global_Sales~.,data=datTrain, C=5)
# B. Generate this model's evaluation metrics on both training and testing data.
prediction_on_train <- predict(svm_model, datTrain)
prediction_on_test <- predict(svm_model, datTest)
mmetric(datTrain$Rider,prediction_on_train, metric="CONF")
mmetric(datTest$Rider,prediction_on_test, metric="CONF")
mmetric(datTrain$Rider,prediction_on_train,metric=c("ACC","PRECISION","TPR","F1"))
mmetric(datTest$Rider,prediction_on_test,metric=c("ACC","PRECISION","TPR","F1"))
mmetric(datTrain$Global_Sales,prediction_on_train, metric="CONF")
mmetric(datTest$Global_Sales,prediction_on_test, metric="CONF")
mmetric(datTrain$Global_Sales,prediction_on_train,metric=c("ACC","PRECISION","TPR","F1"))
mmetric(datTest$Global_Sales,prediction_on_test,metric=c("ACC","PRECISION","TPR","F1"))
mmetric(datTrain_sale$Global_Sales,prediction_on_train,metric = c("MAE","RMSE","MAPE","RAE"))
mmetric(datTest_sale$Global_Sales,prediction_on_test,metric = c("MAE","RMSE","MAPE","RAE"))
mmetric(datTrain$Global_Sales,prediction_on_train,metric = c("MAE","RMSE","MAPE","RAE"))
mmetric(datTest$Global_Sales,prediction_on_test,metric = c("MAE","RMSE","MAPE","RAE"))
### 4. Neural Network for predicting Global_Sales value. (2 points)
# A. Build a MLP model with with N=50, H='8, 4'
library(RWeka)
MLP <- make_Weka_classifier("weka/classifiers/functions/MultilayerPerceptron")
mlp_model <- MLP(Global_Sales~.,data=datTrain)
summary(mlp_model)
# B. Generate this model's evaluation metrics on both training and testing data.
prediction_on_train_mlp <- predict(mlp_model, datTrain)
prediction_on_test_mlp <- predict(mlp_model, datTest)
mmetric(datTrain$Global_Sales,prediction_on_train_mlp,metric = c("MAE","RMSE","MAPE","RAE"))
### 1. Import and clean data
# A. Import data. Load character variable as character strings first (stringsAsFactors = FALSE).
Sales <- read.csv(file = "sales.csv", stringsAsFactors = FALSE)
# B.	Show the overall structure and summary of the input data.
str(Sales)
